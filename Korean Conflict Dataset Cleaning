{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b63df58a-4a10-4248-95f7-10263e8db51c",
    "_uuid": "d2d4922ddba1eaa0c8d90e564464890ed6416c75"
   },
   "source": [
    "# Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Content</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b47c7b1a-54ae-437e-bfed-47a178875054",
    "_uuid": "107dc1b5577e4757eec6b6e0d3d09d9887895706"
   },
   "source": [
    "Geography: Pakistan\n",
    "\n",
    "Time period: 1995-2016\n",
    "\n",
    "Unit of analysis: Attack\n",
    "\n",
    "Dataset: The dataset contains detailed information of 475 bombing attacks in Pakistan that killed an estimated 6,982 and injured 17,624 people.\n",
    "\n",
    "Variables: The dataset contains Serial No, Incident Date, approximate Time, Long-Lat, City, Province, Location, Open/Close Space (as it will change the impact of blast waves due to reflection), min and max number of people killed and injured, amount of explosive being used and the name of hospitals where victims went for treatment.\n",
    "\n",
    "Sources: Unclassified media articles, hospital reports, think tank analysis and reports, and government official press releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "04909a0a-da11-47f5-a477-07583cbdda80",
    "_uuid": "88455d5a6a54c56408936bb5856dc21ed9c83714"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "509b36dc-5596-4ce1-9575-6097f332223d",
    "_uuid": "628b7cf61fd39d8849232c2c6c2eb48881c52875"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'./samples_attacks.csv' does not exist: b'./samples_attacks.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8cc377b2b1e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./samples_attacks.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'./samples_attacks.csv' does not exist: b'./samples_attacks.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./samples_attacks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cddb2d52-f528-41a6-839d-4bda556834c9",
    "_uuid": "7c5a6f10fb1086434f8b1662394a52a540bd13d4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7f778787-45db-46b4-9e66-36a2799f3cf2",
    "_uuid": "58cb360741d0aa3c902dc595bb1a3de161f9ec14"
   },
   "source": [
    "Note that some of the variables got missing values indicated by 'NaN'. But this is not reliable and we need a summary statistics. Let's take a look at list of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a2a2bfa7-f583-45d0-802b-4a1c7ad51878",
    "_uuid": "3a9eb1a1cdec7771cb4e8dcd149cf3470ae29cc4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ade27e2a-0339-472a-930b-848415d58a50",
    "_uuid": "7c6c22f2a9df10bb7290e7318bfa6fd9783e6392"
   },
   "outputs": [],
   "source": [
    "P = df.groupby('Province')['S#'].count().reset_index()\n",
    "\n",
    "P['Percentage'] = 100 * P['S#']  / P['S#'].sum()\n",
    "\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ade27e2a-0339-472a-930b-848415d58a50",
    "_uuid": "7c6c22f2a9df10bb7290e7318bfa6fd9783e6392"
   },
   "outputs": [],
   "source": [
    "P = df.groupby('Location Category')['S#'].count().reset_index()\n",
    "\n",
    "P['Percentage'] = 100 * P['S#']  / P['S#'].sum()\n",
    "\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>I - Sanity Checks</h2>\n",
    "* Check existence of Missing Values - per variable and to what extent -\n",
    "* Data consistency of each variable (temperature, locations, typos, etc.)\n",
    "* Datetime checks\n",
    "* outliers\n",
    "\n",
    "I-\tData Missing Causes\n",
    "1-\tData Gathering:\n",
    "i-\tManual entry (human entry mistakes)\n",
    "ii-\tDifferent resources which can give different information without any format standard\n",
    "iii-\tMore than one person gather the data\n",
    "iv-\tNo information released  \n",
    "\n",
    "2-\tData Delivery: \n",
    "i-\tLoss in transmission\n",
    "ii-\tWrong files\n",
    "iii-\tBuffer overflow ()\n",
    "\n",
    "3-\tData Storage:\n",
    "i-\tThe data size more than the size of the storage unit\n",
    "ii-\tPoor metadata\n",
    "\n",
    "\n",
    "1-\tBlast Day type: Missing because of:\n",
    "i-\tRepetition (working day)\n",
    "ii-\tKnown information (Sunday is holiday)\n",
    "iii-\tUnknown whether working day or holiday.\n",
    "\n",
    "2-\tHoliday Type: \n",
    "i-\tKnown information (ex: Sunday is weekend)\n",
    "ii-\tUnknown\n",
    "iii-\tWorking day is not a holiday\n",
    "\n",
    "3-\tTime: I was not recorded because the time is an essential thing in these accidents and should be available. \n",
    "4-\tLongitude and latitude: \n",
    "i-\tRepetition: The missing longitude and latitude are found in another rows\n",
    "\n",
    "5-\tTemperature : \n",
    "i-\tThe person who gathered the data didn’t see the weather forecast\n",
    "6-\tTarget Type:\n",
    "i-\tDid not mention in the sources (not allowed)\n",
    "ii-\tUndefined whom the target (several kind)\n",
    "Time, hospital name, target cannot be imputed because it does not relate to each other.\n",
    "But the other categories can be imputed such as:\n",
    "1-\tTemperature, killed and injured: we can take the mean of the values of the province \n",
    "2-\tBlast type: the most frequent or the default.\n",
    "3-\tLocation can be put as the general (province one) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>II - Missing Values</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-68b7e8650aa1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get the number of missing data points per column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmissing_values_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# look at the # of missing points in the first ten columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmissing_values_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m26\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# get the number of missing data points per column\n",
    "missing_values_count = df.isnull().sum()\n",
    "\n",
    "# look at the # of missing points in the first ten columns\n",
    "missing_values_count[0:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f432b6fdb5cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\user\\Desktop\\samples_attacks1.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtotal_cells\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtotal_missing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmissing_values_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# percent of data that is missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "#percentage of the values in our dataset were missing to give us a better sense of the scale of this problem:\n",
    "# how many total missing values do we have?\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "missing_values_count=[]\n",
    "df = pd.read_excel (r'C:\\Users\\user\\Desktop\\samples_attacks1.xlsx') \n",
    "total_cells = np.product(df.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "\n",
    "# percent of data that is missing\n",
    "(1.0*total_missing/total_cells) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1- Understand why the data is missing</h4>\n",
    "* Is this value missing becuase it wasn't recorded or becuase it dosen't exist?\n",
    "* Solutions: Dismiss or Impute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-12-7da07a07cfa0>, line 363)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-7da07a07cfa0>\"\u001b[1;36m, line \u001b[1;32m363\u001b[0m\n\u001b[1;33m    df=df.dropna(subset = ['Time', 'Location', 'Location Category', 'Open/Closed Space', 'Target Type', 'Hospital Names'])​\u001b[0m\n\u001b[1;37m                                                                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    " #h42- Dismiss missing values</h4>\n",
    "from datetime import date\n",
    "from dateutil.parser import *\n",
    "from dateutil.tz import *\n",
    "from datetime import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import io, os, sys, types\n",
    "from geopy.geocoders import Nominatim\n",
    "df = pd.read_excel (r'C:\\Users\\user\\Desktop\\samples_attacks1.xlsx') \n",
    "dt_string=df['Date']\n",
    "i=0\n",
    "j=0\n",
    "n=0\n",
    "f=[]\n",
    "f1=[]\n",
    "h1=[]\n",
    "\n",
    "h2=[]\n",
    "\n",
    "h3=[]\n",
    "\n",
    "h4=[]\n",
    "\n",
    "h5=[]\n",
    "\n",
    "h6=[]\n",
    "\n",
    "h7=[]\n",
    "\n",
    "h8=[]\n",
    "\n",
    "h9=[]\n",
    "\n",
    "h10=[]\n",
    "\n",
    "h11=[]\n",
    "\n",
    "h12=[]\n",
    "\n",
    "null_temps=[]\n",
    "\n",
    "mean_temps=[]\n",
    "\n",
    "location=[]\n",
    "\n",
    "new_col1=[]\n",
    "###DATE CLEANING\n",
    "\n",
    "col=df['Temperature(C)'].dropna()\n",
    "\n",
    "##Removing \"-\" from date\n",
    "\n",
    "while i<len(dt_string):\n",
    "\n",
    "    dt_string[i]=dt_string[i].replace(\"-\", \" \")\n",
    "\n",
    "    i=i+1 \n",
    "\n",
    "    \n",
    "\n",
    "##Changing format of dates given\n",
    "\n",
    "while j<len(dt_string):\n",
    "\n",
    "        new_date=parse(dt_string[j])\n",
    "\n",
    "        f.append(new_date)\n",
    "\n",
    "        j=j+1\n",
    "\n",
    "        \n",
    "\n",
    "##Adding new dates to original dataframe\n",
    "\n",
    "while n<len(f):\n",
    "\n",
    "    f1.append(f[n].strftime('%m/%d/%Y'))\n",
    "\n",
    "    n=n+1\n",
    "\n",
    "df['New Date']=f1   \n",
    "# Finding Average Temperature per month\n",
    "\n",
    "i=0\n",
    "\n",
    "j=0\n",
    "\n",
    "try:\n",
    "\n",
    "    for i in range(0, len(col)):\n",
    "\n",
    "        if  f[i].month==1 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h1.append(value)\n",
    "\n",
    "        if  f[i].month==2 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h2.append(value)\n",
    "\n",
    "        if  f[i].month==3 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h3.append(value)\n",
    "\n",
    "        if  f[i].month==4 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h4.append(value)\n",
    "\n",
    "        if  f[i].month==5 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h5.append(value)\n",
    "\n",
    "        if  f[i].month==6 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h6.append(value)\n",
    "\n",
    "        if  f[i].month==7 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h7.append(value)\n",
    "\n",
    "        if  f[i].month==8 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h=h8.append(value)\n",
    "\n",
    "        if  f[i].month==9 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h9.append(value)\n",
    "\n",
    "        if  f[i].month==10 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h10.append(value)\n",
    "\n",
    "        if  f[i].month==11 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h11.append(value)\n",
    "\n",
    "        if  f[i].month==12 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h12.append(value)\n",
    "\n",
    "except KeyError:\n",
    "\n",
    "    pass\n",
    "\n",
    "##Finding Mean for each month based on given data\n",
    "\n",
    "mean_temps=[statistics.mean(h1),statistics.mean(h2),statistics.mean(h3),statistics.mean(h4),statistics.mean(h5),statistics.mean(h6),statistics.mean(h7),statistics.mean(h8),statistics.mean(h9),statistics.mean(h10),statistics.mean(h11),statistics.mean(h12)]\n",
    "\n",
    "null_temps.append(df['New Date'][(df['Temperature(C)'].isna()== 1)])\n",
    "\n",
    "null_t=null_temps[0].index.tolist()\n",
    "\n",
    "\n",
    "\n",
    "##Looping to find the month corresponding to missing temperature and filling with the corresponding mean temp from the vector found above\n",
    "\n",
    "for i in range (0,len(null_t)):\n",
    "\n",
    "    df['Temperature(C)'][null_t[i]]==mean_temps[int(f[null_t[i]].month)]\n",
    "\n",
    "            \n",
    "\n",
    "##Cleaning of City Column ( Removing Case Irregularities and Random Spaces)\n",
    "\n",
    "i=0\n",
    "\n",
    "j=0\n",
    "\n",
    "for i in range(0,len(df['City'])) :\n",
    "\n",
    "    new_col1.append(df['City'][i].capitalize().strip())\n",
    "\n",
    "df['City']=new_col1\n",
    "\n",
    "\n",
    "\n",
    "##Get Unique Values of Cities\n",
    "\n",
    "i=0\n",
    "\n",
    "def unique(list1): \n",
    "\n",
    "    x = np.array(list1) \n",
    "\n",
    "    return(np.unique(x))\n",
    "\n",
    "unique_cities=np.sort(unique(new_col1))\n",
    "\n",
    "\n",
    "\n",
    "##Get Lat,Lng of Cities ( Current free trial only allows usage of Google API only a few times GeocoderTimedOut). Planned touse geocode data to replace missing data using .isna\n",
    "\n",
    "\n",
    "\n",
    "i=0\n",
    "\n",
    "for i in range(0,len(unique_cities)):\n",
    "\n",
    "        location.append(geolocator.geocode(str(unique_cities[i])))\n",
    "\n",
    "#print(location)\n",
    "\n",
    "print(df)\n",
    "\n",
    "print(null_t)\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "from dateutil.parser import *\n",
    "\n",
    "from dateutil.tz import *\n",
    "\n",
    "from datetime import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import statistics\n",
    "\n",
    "import io, os, sys, types\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "df = pd.read_excel (r'C:\\Users\\user\\Desktop\\samples_attacks1.xlsx') \n",
    "\n",
    "dt_string=df['Date']\n",
    "\n",
    "i=0\n",
    "\n",
    "j=0\n",
    "\n",
    "n=0\n",
    "\n",
    "f=[]\n",
    "\n",
    "f1=[]\n",
    "\n",
    "h1=[]\n",
    "\n",
    "h2=[]\n",
    "\n",
    "h3=[]\n",
    "\n",
    "h4=[]\n",
    "\n",
    "h5=[]\n",
    "\n",
    "h6=[]\n",
    "\n",
    "h7=[]\n",
    "\n",
    "h8=[]\n",
    "\n",
    "h9=[]\n",
    "\n",
    "h10=[]\n",
    "\n",
    "h11=[]\n",
    "\n",
    "h12=[]\n",
    "\n",
    "null_temps=[]\n",
    "\n",
    "mean_temps=[]\n",
    "\n",
    "location=[]\n",
    "\n",
    "new_col1=[]\n",
    "\n",
    "###DATE CLEANING(2)\n",
    "\n",
    "col=df['Temperature(C)'].dropna()\n",
    "\n",
    "##Removing \"-\" from date\n",
    "\n",
    "while i<len(dt_string):\n",
    "\n",
    "    dt_string[i]=dt_string[i].replace(\"-\", \" \")\n",
    "\n",
    "    i=i+1 \n",
    "\n",
    "    \n",
    "\n",
    "##Changing format of dates given\n",
    "\n",
    "while j<len(dt_string):\n",
    "\n",
    "        new_date=parse(dt_string[j])\n",
    "\n",
    "        f.append(new_date)\n",
    "\n",
    "        j=j+1\n",
    "\n",
    "        \n",
    "\n",
    "##Adding new mm/dd/yyy dates to original dataframe\n",
    "\n",
    "while n<len(f):\n",
    "\n",
    "    f1.append(f[n].strftime('%m/%d/%Y'))\n",
    "\n",
    "    n=n+1\n",
    "\n",
    "df['New Date']=f1 \n",
    "\n",
    "\n",
    "\n",
    "##Cleaning of City Column ( Removing Case Irregularities and Random Spaces)\n",
    "\n",
    "i=0\n",
    "\n",
    "j=0\n",
    "\n",
    "for i in range(0,len(df['City'])) :\n",
    "\n",
    "    new_col1.append(df['City'][i].capitalize().strip())\n",
    "\n",
    "df['City']=new_col1\n",
    "\n",
    "\n",
    "\n",
    "##Get Unique Values of Cities\n",
    "\n",
    "i=0\n",
    "\n",
    "def unique(list1): \n",
    "\n",
    "    x = np.array(list1) \n",
    "\n",
    "    return(np.unique(x))\n",
    "\n",
    "unique_cities=np.sort(unique(new_col1))\n",
    "\n",
    "\n",
    "#Dismissing Values \n",
    "df=df[1:] = df[1:].replace(0, np.NaN)\n",
    "df=df.dropna(subset = ['Time', 'Location', 'Location Category', 'Open/Closed Space', 'Target Type', 'Hospital Names'])​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-14-e8769e01d8f7>, line 154)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-e8769e01d8f7>\"\u001b[1;36m, line \u001b[1;32m154\u001b[0m\n\u001b[1;33m    df['Killed Max'].fillna((df['Killed Max'].mean()), inplace=True)​\u001b[0m\n\u001b[1;37m                                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "#h43- Impute using three different methods (each for a different variable)</h4>\n",
    "\n",
    "###Data Filling (Temp + Latitude+Longitude)\n",
    "\n",
    "## Finding Average Temperature per month\n",
    "\n",
    "i=0\n",
    "\n",
    "j=0\n",
    "\n",
    "try:\n",
    "\n",
    "    for i in range(0, len(col)):\n",
    "\n",
    "        if  f[i].month==1 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h1.append(value)\n",
    "\n",
    "        if  f[i].month==2 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h2.append(value)\n",
    "\n",
    "        if  f[i].month==3 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h3.append(value)\n",
    "\n",
    "        if  f[i].month==4 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h4.append(value)\n",
    "\n",
    "        if  f[i].month==5 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h5.append(value)\n",
    "\n",
    "        if  f[i].month==6 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h6.append(value)\n",
    "\n",
    "        if  f[i].month==7 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h7.append(value)\n",
    "\n",
    "        if  f[i].month==8 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h=h8.append(value)\n",
    "\n",
    "        if  f[i].month==9 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h9.append(value)\n",
    "\n",
    "        if  f[i].month==10 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h10.append(value)\n",
    "\n",
    "        if  f[i].month==11 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h11.append(value)\n",
    "\n",
    "        if  f[i].month==12 :\n",
    "\n",
    "                     value=col[i]\n",
    "\n",
    "                     h12.append(value)\n",
    "\n",
    "except KeyError:\n",
    "\n",
    "    pass\n",
    "############################################################       MEAN for the temperature\n",
    "##Finding Mean for each month based on given data\n",
    "\n",
    "mean_temps=[statistics.mean(h1),statistics.mean(h2),statistics.mean(h3),statistics.mean(h4),statistics.mean(h5),statistics.mean(h6),statistics.mean(h7),statistics.mean(h8),statistics.mean(h9),statistics.mean(h10),statistics.mean(h11),statistics.mean(h12)]\n",
    "\n",
    "null_temps.append(df['New Date'][(df['Temperature(C)'].isna()== 1)])\n",
    "\n",
    "null_t=null_temps[0].index.tolist()\n",
    "\n",
    "\n",
    "\n",
    "##Looping to find the month corresponding to missing temperature and filling with the corresponding mean temp from the vector found above\n",
    "\n",
    "for i in range (0,len(null_t)):\n",
    "\n",
    "    df['Temperature(C)'][null_t[i]]==mean_temps[int(f[null_t[i]].month)]\n",
    "\n",
    "            \n",
    "\n",
    "##Cleaning of City Column ( Removing Case Irregularities and Random Spaces)\n",
    "\n",
    "i=0\n",
    "\n",
    "j=0\n",
    "\n",
    "for i in range(0,len(df['City'])) :\n",
    "\n",
    "    new_col1.append(df['City'][i].capitalize().strip())\n",
    "\n",
    "df['City']=new_col1\n",
    "\n",
    "\n",
    "\n",
    "##Get Unique Values of Cities\n",
    "\n",
    "i=0\n",
    "\n",
    "def unique(list1): \n",
    "\n",
    "    x = np.array(list1) \n",
    "\n",
    "    return(np.unique(x))\n",
    "\n",
    "unique_cities=np.sort(unique(new_col1))\n",
    "\n",
    "\n",
    "\n",
    "##Get Lat,Lng of Cities ( Current free trial only allows usage of Google API only a few times GeocoderTimedOut). Planned touse geocode data to replace missing data using .isna\n",
    "\n",
    "\n",
    "\n",
    "i=0\n",
    "\n",
    "for i in range(0,len(unique_cities)):\n",
    "\n",
    "        location.append(geolocator.geocode(str(unique_cities[i])))\n",
    "\n",
    "#print(location)\n",
    "\n",
    "print(df)\n",
    "\n",
    "print(null_t)\n",
    "\n",
    "# first method (mean)​\n",
    "df['Killed Max'].fillna((df['Killed Max'].mean()), inplace=True)​\n",
    "df['Killed Min'].fillna((df['Killed Min'].mean()), inplace=True)​\n",
    "df['Injured Max'].fillna((df['Injured Max'].mean()), inplace=True)​\n",
    "df['Injured Min'].fillna((df['Injured Min'].mean()), inplace=True)​\n",
    "\n",
    "#Second Method (ffil)​\n",
    "df1[\"Killed Max\"].fillna( method ='ffill', inplace = True) ​\n",
    "df1[\"Killed Min\"].fillna( method ='ffill', inplace = True) ​\n",
    "df1[\"Temperature(C)\"].fillna( method ='ffill', inplace = True) ​\n",
    "df1[\"Temperature(F)\"].fillna( method ='ffill', inplace = True) ​\n",
    "df1[\"Injured Max\"].fillna( method ='ffill', inplace = True) ​\n",
    "df1[\"Injured Min\"].fillna( method ='ffill', inplace = True) ​\n",
    "\n",
    "# third Method (most frequent)​\n",
    "df2['Killed Max'].fillna((df2['Killed Max'].value_counts().idxmax()), inplace=True)​\n",
    "df2['Killed Min'].fillna((df2['Killed Min'].value_counts().idxmax()), inplace=True)​\n",
    "df2['Injured Min'].fillna((df2['Injured Min'].value_counts().idxmax()), inplace=True)​\n",
    "df2['Injured Max'].fillna((df2['Injured Max'].value_counts().idxmax()), inplace=True)​\n",
    "df2['Temperature(C)'].fillna((df2['Temperature(C)'].value_counts().idxmax()), inplace=True)​\n",
    "df2['Temperature(F)'].fillna((df2['Temperature(F)'].value_counts().idxmax()), inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-3e92d9fe6380>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-3e92d9fe6380>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    <h4>4- Comparing All Solutions</h4>\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<h4>4- Comparing All Solutions</h4>\n",
    "\n",
    "\n",
    "#Compare​\n",
    "\n",
    "df3 = df[['Injured Min']]​\n",
    "df4 = df1[['Injured Min']]​\n",
    "df5 = df2[['Injured Min']]​\n",
    "df8 = pd.concat([df3,df4,df5], axis=1)​\n",
    "df8.insert(loc=0, column='A', value=np.arange(len(df8)))​\n",
    "df8.columns = ['A','mean','Ffill', 'frequent']​\n",
    "df8.plot(x=\"A\", y=['mean','Ffill', 'frequent'], figsize=(10,5), grid=True)​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>III - Inconsistencies</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1- Detect Inconsistencies by looking into typos of names, mismatch in locations, etc. </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2- Solve Inconsistencies by looking into typos of names, mismatch in locations, etc. by matching items.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>IV - Outliers</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<h4> 1- Identifying outliers by looking at distributions (time series, control charts, etc.)</h4>\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_excel (r'C:\\Users\\USER1\\Desktop\\samples_attacks.xlsx') \n",
    "df.boxplot(['Killed Max','Killed Min','Injured Min','Temperature(C)','Temperature(F)'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h4>2- Understand why these otliers exist</h4>\n",
    "* Glitches or legitimate \n",
    "\n",
    "An outlier may be due to variability in the measurement.\n",
    "There are a number of reasons for outliers:\n",
    "  1.  Some individuals in the sample are extreme (True Values);\n",
    "  2.  The data are inappropriately scaled (false Values);\n",
    "  3.  Errors were made on data entry (Manual entry);\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<h4>3- How would you handle these outliers?</h4>\n",
    "\n",
    "1-Handle true outliers by caping them. For example, High value of killed people  behave in the same way as lower value. In this case, we can disregard the value at a certain level that keeps the data intact. /n\n",
    "2-If the value seems to be due to a mistake, we can solve the outlier problem by just imputing a new value /n\n",
    "3-Drop the outliers value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
