{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS2_Spark_Stream_개인연습.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/imsangil/ProgrammingAssignment2/blob/master/DS2_Spark_Stream_%EA%B0%9C%EC%9D%B8%EC%97%B0%EC%8A%B5.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Zhde7x_etP1w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 0: Environment setup"
      ]
    },
    {
      "metadata": {
        "id": "J-TdhkUDbGfc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "00e1804d-5456-4a57-9a6c-bac0c660e071"
      },
      "cell_type": "code",
      "source": [
        "# Setting up spark\n",
        "!rm -rf /content/*\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.mirror.cdnetworks.com/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7.tgz\n",
        "!tar -xf /content/spark-2.3.2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "# Download necessary dependency file for Kafka\n",
        "!wget -q http://central.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.3.2/spark-streaming-kafka-0-8-assembly_2.11-2.3.2.jar\n",
        "!ls /content/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu artful-security InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.149)]\r0% [1 InRelease gpgv 83.2 kB] [Connecting to archive.ubuntu.com (91.189.88.149)\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu artful InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu artful-updates InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu artful-backports InRelease\n",
            "Reading package lists... Done\n",
            "\n",
            "Redirecting output to ‘wget-log’.\n",
            "\n",
            "Redirecting output to ‘wget-log.1’.\n",
            "spark-2.3.2-bin-hadoop2.7\t\t\t   wget-log\n",
            "spark-2.3.2-bin-hadoop2.7.tgz\t\t\t   wget-log.1\n",
            "spark-streaming-kafka-0-8-assembly_2.11-2.3.2.jar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vzzBCUnufKQs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Setting the environment variable\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.2-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TicPX46ymRGZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vm4P6Vyqjcob",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1: Running a simple wordcount query"
      ]
    },
    {
      "metadata": {
        "id": "LNPNeB_-t5Yp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will implement a simple continuous wordcount query. This query will\n",
        "* Read the text sentence from a Kafka\n",
        "* Split the sentence into words\n",
        "* Continuously aggregate the counts for each word\n",
        "\n",
        "Firstly, we need to start from making a simple TCP server on the master server which produces random sentences to its clients. In this class, we will use `nc (netcat)` program. You will get the data stream from the TCP socket server provided by TA."
      ]
    },
    {
      "metadata": {
        "id": "cfURCAGS7-Je",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "UMpTF-Enfr10",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import os\n",
        "import json\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkConf\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# Create a spark context\n",
        "conf = SparkConf().setAppName(\"WordCount\").setMaster(\"local[*]\").set(\"spark.jars\", \"/content/spark-streaming-kafka-0-8-assembly_2.11-2.3.2.jar\")\n",
        "spark_context = SparkContext(conf=conf)\n",
        "# Create a streaming context with batch interval 5 secs\n",
        "stream_context = StreamingContext(spark_context, 5)\n",
        "# Read the text line from the socket stream\n",
        "lines = stream_context.socketTextStream(\"147.46.216.122\", 20332)\n",
        "# Split each line into multiple words\n",
        "words = lines.flatMap(lambda x: x.split(\" \"))\n",
        "# Make a (key(word), count) stream and Count the word\n",
        "word_counts = words.map(lambda x: (x, 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "    \n",
        "word_counts.pprint()\n",
        "\n",
        "stream_context.start()\n",
        "# Wait for 60 seconds\n",
        "stream_context.awaitTermination(60)\n",
        "stream_context.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DHlE14SPvThR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stream_context.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "03n8L8bqvnh4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2: Running a stream application from Kafka source\n",
        "\n",
        "Apache Kafka is a distributed streaming platform which supports messaging, processing, and storing of the stream data. In this practice session, we will focus on leveraging Kafka as a message brokering system.\n",
        "\n",
        "Kafka supports high-throughput & fault-tolerant messaging via publish-subscribe model. In publish-subscribe model, stream events are managed in **topics**. A **Producer** consistently generates a data, whereas **Consumer** receives the data events. Each topic is partitioned into multiple \"partitions\", and partitions are distributed and stored in the secondary storage to guarantee fault tolerance.\n",
        "\n",
        "As we can guess from the information above, we need the server address and topic name to fetch the data from a Kafka broker. Kafka server and producers are already set up by TAs. We will review the Producer code firstly.\n",
        "\n",
        "After revewing the code, we will implement the same word count application from the Kafka source. The broker address is **147.46.216.122:9092** and the topic is **wc**."
      ]
    },
    {
      "metadata": {
        "id": "uDJvbziXg0_7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "ed7298ea-6215-411e-b553-35b60326692f"
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import os\n",
        "import json\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkConf\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "\n",
        "# Create a spark context containing the additional Kafka dependency.\n",
        "conf = SparkConf().setAppName(\"KafkaWordCount\").setMaster(\"local[*]\").set(\"spark.jars\", \"/content/spark-streaming-kafka-0-8-assembly_2.11-2.3.2.jar\")\n",
        "spark_context = SparkContext(conf=conf)\n",
        "# Create a streaming context with batch interval 5 secs\n",
        "stream_context = StreamingContext(spark_context, 5)\n",
        "# Read the Kafka data stream from the broker for the given topic.\n",
        "# The data will be arrived in (topic, data) format\n",
        "kvs = KafkaUtils.createDirectStream(stream_context, [\"wc\"], {\"metadata.broker.list\": \"147.46.216.122:9092\"})\n",
        "# Extract the data from the Kafka data stream\n",
        "lines = kvs.map(lambda x: x[1]).map(lambda l: l.replace('\"', ''))\n",
        "# Split each line into multiple words\n",
        "words = lines.flatMap(lambda x: x.split(\" \"))\n",
        "# Count the word\n",
        "word_counts = words.map(lambda x: (x, 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "word_counts.pprint()\n",
        "\n",
        "# Start the computation\n",
        "stream_context.start()\n",
        "# await termination for 60 seconds\n",
        "stream_context.awaitTermination(20)\n",
        "stream_context.stop()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Time: 2018-09-28 02:09:35\n",
            "-------------------------------------------\n",
            "(u'fun', 3)\n",
            "(u'is', 3)\n",
            "(u'DS2', 5)\n",
            "(u'session', 2)\n",
            "(u'to', 2)\n",
            "(u'practice', 2)\n",
            "(u'Welcome', 2)\n",
            "(u'class', 3)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2018-09-28 02:09:40\n",
            "-------------------------------------------\n",
            "(u'a', 1)\n",
            "(u'is', 1)\n",
            "(u'DS2', 1)\n",
            "(u'Have', 1)\n",
            "(u'fun', 1)\n",
            "(u'expired', 2)\n",
            "(u'Session', 2)\n",
            "(u'nice', 1)\n",
            "(u'class', 1)\n",
            "(u'day', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2018-09-28 02:09:45\n",
            "-------------------------------------------\n",
            "(u'expired', 3)\n",
            "(u'DS2', 2)\n",
            "(u'to', 2)\n",
            "(u'practice', 2)\n",
            "(u'session', 2)\n",
            "(u'Welcome', 2)\n",
            "(u'Session', 3)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2018-09-28 02:09:50\n",
            "-------------------------------------------\n",
            "(u'a', 2)\n",
            "(u'is', 1)\n",
            "(u'DS2', 1)\n",
            "(u'Have', 2)\n",
            "(u'fun', 1)\n",
            "(u'expired', 2)\n",
            "(u'Session', 2)\n",
            "(u'nice', 2)\n",
            "(u'day', 2)\n",
            "(u'class', 1)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vrmM1vFm8Uf7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stream_context.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C6NPsD7Fv7Rb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Until now, we processed only the simple plain texts. From this time, we will process JSON-formatted data events which are widely used for data transfer. Here, we will get the json-formatted movie datasets from the kafka server. We can get the data from the **movie** topic.\n",
        "\n",
        "To  json-formatted data, we will use python json `json` package. You can get the python dictionary instance by calling `json.loads` on serialized json data."
      ]
    },
    {
      "metadata": {
        "id": "cLk1JX38hZk-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import os\n",
        "import json\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkConf\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "\n",
        "# Create a spark context\n",
        "conf = SparkConf().setAppName(\"movie_json\").setMaster(\"local[*]\") \\\n",
        "  .set(\"spark.jars\", \"/content/spark-streaming-kafka-0-8-assembly_2.11-2.3.2.jar\")\n",
        "spark_context = SparkContext(conf=conf)\n",
        "# Create a streaming context with batch interval 5 secs\n",
        "stream_context = StreamingContext(spark_context, 5)\n",
        "# Get the kafka stream data\n",
        "kvs = KafkaUtils.createDirectStream(stream_context, [\"movie\"], {\"metadata.broker.list\": \"147.46.216.122:9092\"})\n",
        "# Deserialize the json-formattted data into python dict.\n",
        "movies = kvs.map(lambda x: json.loads(x[1]))\n",
        "# Print the input data\n",
        "movies.pprint()\n",
        "\n",
        "# Start the computation\n",
        "stream_context.start()\n",
        "# await termination for 60 seconds\n",
        "stream_context.awaitTermination(20)\n",
        "stream_context.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VMpnU-da8XB-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stream_context.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "svx-2UjPwLVI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can also perform some filtering on json data. For example, you can filter out the movies prodcued before 2000 from the code below."
      ]
    },
    {
      "metadata": {
        "id": "vm2FwhYAwOpt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import os\n",
        "import json\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkConf\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "\n",
        "# Create a spark context\n",
        "conf = SparkConf().setAppName(\"filter_movie\").setMaster(\"local[*]\").set(\"spark.jars\", \"/content/spark-streaming-kafka-0-8-assembly_2.11-2.3.2.jar\")\n",
        "spark_context = SparkContext(conf=conf)\n",
        "# Create a streaming context with batch interval 5 secs\n",
        "stream_context = StreamingContext(spark_context, 5)\n",
        "#lines = stream_context.socketTextStream(\"master\", 5000)\n",
        "kvs = KafkaUtils.createDirectStream(stream_context, [\"movie\"], {\"metadata.broker.list\": \"147.46.216.122:9092\"})\n",
        "# Deserialize the json-formattted data into python dict.\n",
        "movies = kvs.map(lambda x: json.loads(x[1]))\n",
        "# Filter out movies produced before 2000.\n",
        "twentyfirstcentry_movies = movies.filter(lambda movie: movie['year'] >= 2000)\n",
        "# Print the input data\n",
        "twentyfirstcentry_movies.pprint()\n",
        "\n",
        "# Start the computation\n",
        "stream_context.start()\n",
        "# await termination for 60 seconds\n",
        "stream_context.awaitTermination(60)\n",
        "stream_context.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l2kRiY5q8YZf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stream_context.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SQI2nwutwYQ4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quiz 0: Filter & Map\n",
        "\n",
        "In the following cell, implement the word count example which gets the data from the **\"wc\"** topic according to the following condition\n",
        "* Filters out the word 'DS2'\n",
        "* Double the frequency of the word 'class'\n",
        "\n",
        "Hint: Make a separate method rather than lambda for filtering for easy coding.\n",
        "```\n",
        "def method(word):\n",
        "   ...\n",
        "   \n",
        "stream.map(method)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "0v5Cthj-h1Ou",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import os\n",
        "import json\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkConf\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "\n",
        "# Create a spark context\n",
        "conf = SparkConf().setAppName(\"quiz_0\").setMaster(\"local[*]\").set(\"spark.jars\", \"/content/spark-streaming-kafka-0-8-assembly_2.11-2.3.2.jar\")\n",
        "spark_context = SparkContext(conf=conf)\n",
        "# Create a streaming context with batch interval 5 secs\n",
        "stream_context = StreamingContext(spark_context, 5)\n",
        "kvs = KafkaUtils.createDirectStream(stream_context, [\"wc\"], {\"metadata.broker.list\": \"147.46.216.122:9092\"})\n",
        "\n",
        "# TODO: Implement your codes here!\n",
        "words = kvs.map(lambda x: json.loads(x[1]))\n",
        "\n",
        "# Start the computation\n",
        "stream_context.start()\n",
        "# await termination for 60 seconds\n",
        "stream_context.awaitTermination(60)\n",
        "stream_context.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WCELH7su8Za-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stream_context.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_gQAW0F5whtD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quiz 1: Filtering on JSON-formatted data\n",
        "\n",
        "In the following cell, implement the stream application which receives the **movie** topic from the Kafka stream and filters out all the movies which does not contain the word \"the\" in their (cases are ignored). Print out the titles of the movies which are not filtered out.\n",
        "\n",
        "## Example\n",
        "\n",
        "**Input**: {\"title\": \"The titanic\", ...}, {\"title\": \"Titanic\", ...}, {\"title\": \"Flintheart Glomgold\", ...}, ...\n",
        "\n",
        "**Output**: \"The titanic\", \"Flintheart Glombold\", ...\n",
        "\n",
        "Hint: Use `.lower()` method and `in` operator.\n",
        "Ex) \n",
        "```\n",
        ">>> a = \"Hello\" \n",
        ">>> a.lower() \n",
        "\"hello\"\n",
        ">>> \"llo\" in a\n",
        "True\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "hDj4gI0awp7t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import os\n",
        "import json\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkConf\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "\n",
        "# Create a spark context\n",
        "conf = SparkConf().setAppName(\"quiz_1\").setMaster(\"local[*]\").set(\"spark.jars\", \"/content/spark-streaming-kafka-0-8-assembly_2.11-2.3.2.jar\")\n",
        "spark_context = SparkContext(conf=conf)\n",
        "# Create a streaming context with batch interval 5 secs\n",
        "stream_context = StreamingContext(spark_context, 5)\n",
        "# Get the data from the Kafka Stream\n",
        "kvs = KafkaUtils.createDirectStream(stream_context, [\"movie\"], {\"metadata.broker.list\": \"147.46.216.122:9092\"})\n",
        "\n",
        "# TODO: Implement your code here!\n",
        "\n",
        "# Deserialize the json-formattted data into python dict.\n",
        "movies = kvs.map(lambda x: json.loads(x[1]))\n",
        "\n",
        "# Start the computation\n",
        "stream_context.start()\n",
        "# await termination for 60 seconds\n",
        "stream_context.awaitTermination(60)\n",
        "stream_context.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nvDjFADb8ab-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stream_context.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3_RK6Ov3wzbQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 3: Running a windowed stream application\n",
        "\n",
        "By windowing, we can continuously get the set of recent data. A time-based **sliding window** can be defined by **window size** and **sliding interval**. For example, the window of `(window size = 5 seconds, sliding interval = 1 seconds)` consistently emits the data events in recent five seconds for every one second. For the special cases when the window size and the sliding interval is same, we call them as **tumbling windows**.\n",
        "\n",
        "Let's make a windowed movie stream, with window size of 30 second and sliding interval of 5 second."
      ]
    },
    {
      "metadata": {
        "id": "h5yax1Sfw1wX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "5b4ae4f8-9b50-4663-8cdd-46bf5fe4c91b"
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import os\n",
        "import json\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkConf\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "\n",
        "# Create a spark context\n",
        "conf = SparkConf().setAppName(\"window\").setMaster(\"local[*]\").set(\"spark.jars\", \"/content/spark-streaming-kafka-0-8-assembly_2.11-2.3.2.jar\")\n",
        "spark_context = SparkContext(conf=conf)\n",
        "# Create a streaming context with batch interval 5 secs\n",
        "stream_context = StreamingContext(spark_context, 5)\n",
        "#lines = stream_context.socketTextStream(\"master\", 5000)\n",
        "kvs = KafkaUtils.createDirectStream(stream_context, [\"movie\"], {\"metadata.broker.list\": \"147.46.216.122:9092\"})\n",
        "# Deserialize the json-formattted data into python dict.\n",
        "movies = kvs.map(lambda x: json.loads(x[1]))\n",
        "# Make a time-based window (size = 30 secs, interval = 5 secs)\n",
        "windowed_movies = movies.window(30, 5)\n",
        "# Print the window\n",
        "windowed_movies.pprint()\n",
        "\n",
        "# Start the computation\n",
        "stream_context.start()\n",
        "# await termination for 60 seconds\n",
        "stream_context.awaitTermination(20)\n",
        "stream_context.stop()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Time: 2018-09-28 02:11:30\n",
            "-------------------------------------------\n",
            "{u'genre': u'Animated short', u'title': u'Lambert the Sheepish Lion', u'year': 1951}\n",
            "{u'genre': u'Drama', u'title': u'Is There Life Out There?', u'year': 1994}\n",
            "{u'genre': u'Romantic comedy', u'title': u\"It's Complicated\", u'year': 2009}\n",
            "{u'genre': u'Documentary', u'title': u'Winter Soldier', u'year': 1972}\n",
            "{u'genre': u'Crime', u'title': u'Tell No Tales', u'year': 1939}\n",
            "{u'genre': u'Drama', u'title': u'The Craving', u'year': 1916}\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2018-09-28 02:11:35\n",
            "-------------------------------------------\n",
            "{u'genre': u'Animated short', u'title': u'Lambert the Sheepish Lion', u'year': 1951}\n",
            "{u'genre': u'Drama', u'title': u'Is There Life Out There?', u'year': 1994}\n",
            "{u'genre': u'Romantic comedy', u'title': u\"It's Complicated\", u'year': 2009}\n",
            "{u'genre': u'Documentary', u'title': u'Winter Soldier', u'year': 1972}\n",
            "{u'genre': u'Crime', u'title': u'Tell No Tales', u'year': 1939}\n",
            "{u'genre': u'Drama', u'title': u'The Craving', u'year': 1916}\n",
            "{u'genre': u'Animation', u'title': u'Free Jimmy', u'year': 2006}\n",
            "{u'genre': u'Crime drama', u'title': u'16 Blocks', u'year': 2006}\n",
            "{u'genre': u'Sci-fi', u'title': u'Cyborg 2087', u'year': 1966}\n",
            "{u'genre': u'Comedy', u'title': u'Daddy Long Legs', u'year': 1931}\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2018-09-28 02:11:40\n",
            "-------------------------------------------\n",
            "{u'genre': u'Animated short', u'title': u'Lambert the Sheepish Lion', u'year': 1951}\n",
            "{u'genre': u'Drama', u'title': u'Is There Life Out There?', u'year': 1994}\n",
            "{u'genre': u'Romantic comedy', u'title': u\"It's Complicated\", u'year': 2009}\n",
            "{u'genre': u'Documentary', u'title': u'Winter Soldier', u'year': 1972}\n",
            "{u'genre': u'Crime', u'title': u'Tell No Tales', u'year': 1939}\n",
            "{u'genre': u'Drama', u'title': u'The Craving', u'year': 1916}\n",
            "{u'genre': u'Animation', u'title': u'Free Jimmy', u'year': 2006}\n",
            "{u'genre': u'Crime drama', u'title': u'16 Blocks', u'year': 2006}\n",
            "{u'genre': u'Sci-fi', u'title': u'Cyborg 2087', u'year': 1966}\n",
            "{u'genre': u'Comedy', u'title': u'Daddy Long Legs', u'year': 1931}\n",
            "...\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2018-09-28 02:11:45\n",
            "-------------------------------------------\n",
            "{u'genre': u'Animated short', u'title': u'Lambert the Sheepish Lion', u'year': 1951}\n",
            "{u'genre': u'Drama', u'title': u'Is There Life Out There?', u'year': 1994}\n",
            "{u'genre': u'Romantic comedy', u'title': u\"It's Complicated\", u'year': 2009}\n",
            "{u'genre': u'Documentary', u'title': u'Winter Soldier', u'year': 1972}\n",
            "{u'genre': u'Crime', u'title': u'Tell No Tales', u'year': 1939}\n",
            "{u'genre': u'Drama', u'title': u'The Craving', u'year': 1916}\n",
            "{u'genre': u'Animation', u'title': u'Free Jimmy', u'year': 2006}\n",
            "{u'genre': u'Crime drama', u'title': u'16 Blocks', u'year': 2006}\n",
            "{u'genre': u'Sci-fi', u'title': u'Cyborg 2087', u'year': 1966}\n",
            "{u'genre': u'Comedy', u'title': u'Daddy Long Legs', u'year': 1931}\n",
            "...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jrhYN1418cDP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stream_context.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4pzkV1ztxQcL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quiz 2: Windowed aggregation\n",
        "\n",
        "Windowed streams can also be aggregated like normal stream data. Make the windowed stream of movie data firstly (size: 30 secs, interval: 5 secs). On the windowed stream, make a python `dict` whose key is the first alphabet of the movie title and the value is the list of movie titles starting with the alphabet. Before aggregation, make sure that all the movie titles are lower-cased.\n",
        "\n",
        "## Example\n",
        "\n",
        "Input data in window: {\"title\": \"titanic\", ...}, {\"title\": \"Harry Porter\", ...}, {\"title\": \"The Purchase Price\"}\n",
        "\n",
        "Output: {\"h\": \\[\"harry porter\"\\], \"t\": \\[\"titanic\", \"the purchace price\"\\]}\n",
        "\n",
        "Hint: Make key-value stream first using `map()` before making a windowed stream. Also, use list appending in Python.\n",
        "```\n",
        ">>> a = [1, 2, 3]\n",
        ">>> b = [4, 5]\n",
        ">>> a + b\n",
        "[1, 2, 3, 4, 5]\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "9PrDCO8NxWw2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import os\n",
        "import json\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkConf\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "\n",
        "# Create a spark context\n",
        "conf = SparkConf().setAppName(\"quiz_2\").setMaster(\"local[*]\").set(\"spark.jars\", \"/content/spark-streaming-kafka-0-8-assembly_2.11-2.3.2.jar\")\n",
        "spark_context = SparkContext(conf=conf)\n",
        "# Create a streaming context with batch interval 5 secs\n",
        "stream_context = StreamingContext(spark_context, 5)\n",
        "\n",
        "# TODO: Implement your code here!\n",
        "kvs = KafkaUtils.createDirectStream(stream_context, [\"movie\"], {\"metadata.broker.list\": \"147.46.216.122:9092\"})\n",
        "# Deserialize the json-formattted data into python dict.\n",
        "movies = kvs.map(lambda x: json.loads(x[1]))\n",
        "\n",
        "# Start the computation\n",
        "stream_context.start()\n",
        "# Await termination for 60 seconds\n",
        "stream_context.awaitTermination(60)\n",
        "stream_context.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vnqN0jXm8s-I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stream_context.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "evU9FYOsZ_Dc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 4: Running a structured stream application\n",
        "\n",
        "Spark **structured stream** offers high-level stream processing using Spark's **Dataframe** API. Using structured stream, we can apply SQL-like operations on continuously incoming datastreams.\n",
        "Due to the limitation of jupyter notebook, we will perform the practice session on linux terminal. Before we start this session, prepare a new Jupyter terminal which would be used for executing python scripts.\n",
        "\n",
        "## Setup\n",
        "```\n",
        "cd ~\n",
        "/home/ubuntu/spark_scripts/start_cluster.sh\n",
        "git clone https://github.com/DifferentSC/gw-stream-script.git\n",
        "```\n",
        "\n",
        "Below is the running example for the TCP-word count using structured stream.\n",
        "\n",
        "You can execute this code by the following command\n",
        "`/home/ubuntu/spark/bin/spark-submit /home/ubuntu/gw-stream-script/structured_wc.py`\n",
        "\n",
        "To see the result, open another terminal from your notebook, run `nc -lk 20332`, and type random sentences.\n",
        "\n",
        "**NOTE: Do not run your code on Jupyter notebook. Run it on your terminal instead**"
      ]
    },
    {
      "metadata": {
        "id": "kOdN2uRkaDNW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "findspark.init('/home/ubuntu/spark')\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "from pyspark.sql.functions import split\n",
        "\n",
        "## Make a spark sql session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StructuredSocketWordCount\") \\\n",
        "    .master('spark://master:7077') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "## Get the spark data from TCP socket\n",
        "lines = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"socket\") \\\n",
        "    .option(\"host\", \"master\") \\\n",
        "    .option(\"port\", 20332) \\\n",
        "    .load()\n",
        "\n",
        "## Make the new column \"word\" by splitting the line\n",
        "words = lines.select(\n",
        "    explode(\n",
        "        split(lines.value, \" \")\n",
        "    ).alias(\"word\")\n",
        ")\n",
        "\n",
        "wordCounts = words.groupBy(\"word\").count()\n",
        "\n",
        "query = wordCounts \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TRpxijiPaQei",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using `structured stream`, you can also easily consume JSON-formatted data from Kafka stream, and perform relational operations (selection, projection, ...) on them. Here is a sample code which selects the `title` of the movies whose `year` is less than 2000. To run this code, you need to add additional kafka dependency. The running script would be `/home/ubuntu/spark/bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.2 /home/ubuntu/gw-stream-script/structured_json.py`.\n",
        "\n",
        "**NOTE: Do not run your code on Jupyter notebook. Run it on your terminal instead**"
      ]
    },
    {
      "metadata": {
        "id": "AmA3ruxkao7r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "findspark.init('/home/ubuntu/spark')\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "from pyspark.sql.functions import split\n",
        "from pyspark.sql.functions import col, from_json\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "## Make a spark sql session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StructuredSocketWordCount\") \\\n",
        "    .master('spark://master:7077') \\\n",
        "    .getOrCreate()\n",
        "    \n",
        "## Get the json-formatted data from Kafka stream\n",
        "kafka_movies = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\")\n",
        "    .option(\"kafka.bootstrap.servers\", \"147.46.216.122:9092\")\n",
        "    .option(\"subscribe\", \"movie\")\n",
        "    .load()\n",
        "    \n",
        "## Define the relational schema\n",
        "schema = StructType([\n",
        "    StructField(\"title\", StringType()),\n",
        "    StructField(\"genre\", StringType()),\n",
        "    StructField(\"year\", IntegerType())\n",
        "])\n",
        "\n",
        "## Change the JSON events into relational tuples\n",
        "relational_movies = kafka_movies.select(from_json(col(\"value\").cast(\"string\"), schema))\n",
        "\n",
        "## Select the movie titles with year < 2000\n",
        "results = relational_movies.select(\"title\").where(\"year < 2000\")\n",
        "\n",
        "query = results \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}